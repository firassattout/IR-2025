import numpy as np
import os
import joblib
from src.representation.tfidf_model import load_tfidf_model
from src.representation.bert_model import load_bert_embeddings
from src.representation.tfidf_model import load_tfidf_model, transform_tfidf_query
from src.representation.bert_model import load_bert_embeddings, transform_bert_query

def build_hybrid_model(
    tfidf_model_path,
    tfidf_vector_path,
    svd_path,
    bert_model_path,
    bert_vector_path,
    hybrid_vector_path,
    alpha=0.5,
    n_components=384
):
    """Ø¨Ù†Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù‡Ø¬ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF (Ù…Ø¹ SVD) ÙˆBERT."""

    # Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ù„Ù‚Ø©
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    tfidf_model_path = os.path.join(project_root, tfidf_model_path)
    tfidf_vector_path = os.path.join(project_root, tfidf_vector_path)
    svd_path = os.path.join(project_root, svd_path)
    bert_model_path = os.path.join(project_root, bert_model_path)
    bert_vector_path = os.path.join(project_root, bert_vector_path)
    hybrid_vector_path = os.path.join(project_root, hybrid_vector_path)

    # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª TF-IDF
    tfidf_vectorizer, tfidf_matrix, svd_model, tfidf_doc_ids = load_tfidf_model(
        tfidf_model_path, tfidf_vector_path, svd_path
    )

    # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
    print("ğŸ”„ ØªÙ‚Ù„ÙŠÙ„ Ø£Ø¨Ø¹Ø§Ø¯ TF-IDF Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SVD...")
    tfidf_reduced = svd_model.transform(tfidf_matrix)
    print(f"âœ… Ø´ÙƒÙ„ Ù…ØµÙÙˆÙØ© TF-IDF Ø¨Ø¹Ø¯ Ø§Ù„ØªØ®ÙÙŠØ¶: {tfidf_reduced.shape}")

    # ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª BERT
    tokenizer, bert_model, bert_embeddings, bert_doc_ids = load_bert_embeddings(
        bert_model_path, bert_vector_path
    )

    if len(bert_doc_ids) > len(tfidf_doc_ids) and bert_doc_ids[0] == bert_doc_ids[1]:
        print("âš ï¸ Ø¥Ø²Ø§Ù„Ø© Ø£ÙˆÙ„ Ø¹Ù†ØµØ± Ù…ÙƒØ±Ø± Ù…Ù† BERT doc_ids Ùˆembeddings")
        bert_doc_ids = bert_doc_ids[1:]
        bert_embeddings = bert_embeddings[1:]

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØªØ±ØªÙŠØ¨
    if tfidf_doc_ids != bert_doc_ids:
        print("âš ï¸ ØªØ­Ø°ÙŠØ±: ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚. Ø³ÙŠØªÙ… Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ù„ÙƒÙ† ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚.")

    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    print("ğŸ§ª ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª...")
    tfidf_norm = tfidf_reduced / (np.linalg.norm(tfidf_reduced, axis=1, keepdims=True) + 1e-10)
    bert_norm = bert_embeddings / (np.linalg.norm(bert_embeddings, axis=1, keepdims=True) + 1e-10)

    # Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    print("ğŸ”— Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… alpha =", alpha)
    hybrid_vectors = alpha * tfidf_norm + (1 - alpha) * bert_norm

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    os.makedirs(os.path.dirname(hybrid_vector_path), exist_ok=True)
    joblib.dump(hybrid_vectors, hybrid_vector_path)
    joblib.dump(tfidf_doc_ids, f"{hybrid_vector_path}_doc_ids")

    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù‡Ø¬ÙŠÙ†Ø© ÙÙŠ: {hybrid_vector_path}")
    
def transform_hybrid_query(tfidf_vectorizer, svd_model, bert_tokenizer, bert_model, query, alpha=0.5):

    tfidf_query_vec, cleaned_text = transform_tfidf_query(tfidf_vectorizer, query)
    tfidf_query_vec_dense = tfidf_query_vec.toarray()  # ØªØ­ÙˆÙŠÙ„ sparse Ø¥Ù„Ù‰ dense
    tfidf_query_reduced = svd_model.transform(tfidf_query_vec_dense)  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
    tfidf_query_norm = tfidf_query_reduced / (np.linalg.norm(tfidf_query_reduced, axis=1, keepdims=True) + 1e-10)


    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT
    bert_query_vec,cleaned_query = transform_bert_query(bert_tokenizer, bert_model, query)
    bert_query_norm = bert_query_vec / (np.linalg.norm(bert_query_vec, axis=1, keepdims=True) + 1e-10)

    # Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    hybrid_query_vec = alpha * tfidf_query_norm + (1 - alpha) * bert_query_norm

    return hybrid_query_vec,cleaned_text

